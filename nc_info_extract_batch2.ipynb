{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb4edfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BASE_DIR=Y:\\Mingyue\\Lucinda\n",
      "MAIN_DIR=Y:\\Mingyue\\Lucinda\\l2gen_product\n",
      "SUMMARY_CSV=Y:\\Mingyue\\Lucinda\\Lucinda_l2_summary.csv\n",
      "INSITU_CSV=Y:\\Mingyue\\Lucinda\\Lucinda_in_situ_data.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: setup (imports + parameters) ---\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import Optional, Tuple\n",
    "import re\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import netCDF4 as netcdf\n",
    "from netCDF4 import num2date\n",
    "\n",
    "# ============== EDIT THESE IF NEEDED ==============\n",
    "SITE        = \"Lucinda\"\n",
    "TARGET_LAT  = -18.5198          # site latitude\n",
    "TARGET_LON  = 146.386         # site longitude\n",
    "HOURS_TOL   = 3                  # ± hours for the in-situ time-window match\n",
    "\n",
    "# Base folder that contains your .nc files and CSVs\n",
    "BASE_DIR    = Path(\"Y:/\") / \"Mingyue\" / SITE\n",
    "\n",
    "# Folder with many *.nc (SeaDAS L2) files\n",
    "MAIN_DIR    = BASE_DIR / \"l2gen_product\"\n",
    "\n",
    "# Output summary CSV from Cell 2\n",
    "SUMMARY_CSV = BASE_DIR / f\"{SITE}_l2_summary.csv\"\n",
    "\n",
    "# In-situ CSV (must contain columns like Date, Time, Chlorophyll-a)\n",
    "INSITU_CSV  = BASE_DIR / f\"{SITE}_in_situ_data.csv\"\n",
    "\n",
    "# Pattern to find NetCDF files\n",
    "NC_GLOB_PATTERN = \"*.nc\"\n",
    "\n",
    "# Whether to recurse into subfolders of MAIN_DIR\n",
    "RECURSIVE_SEARCH = True\n",
    "# ==================================================\n",
    "\n",
    "# ---- Known aliases ----\n",
    "LAT_ALIASES    = ['latitude', 'lat', 'Latitude']\n",
    "LON_ALIASES    = ['longitude', 'lon', 'Longitude']\n",
    "CHLOR_A_ALIASES= ['chlor_a', 'CHLORA', 'chlorophyll', 'chl_ocx', 'chl']\n",
    "FLAGS_ALIASES  = ['l2_flags', 'l2flags', 'flags']\n",
    "\n",
    "# ---- Common SeaDAS l2_flags bits (adjust if your product differs) ----\n",
    "L2_FLAG_BITS = {\n",
    "    0: \"ATMFAIL\", 1: \"LAND\", 2: \"PRODWARN\", 3: \"HIGLINT\", 4: \"HILT\",\n",
    "    5: \"HISATZEN\", 6: \"COASTZ\", 7: \"SPARE1\", 8: \"STRAYLIGHT\", 9: \"CLDICE\",\n",
    "    10: \"COCCOLITH\", 11: \"TURBIDW\", 12: \"HISOLZEN\", 13: \"SPARE2\", 14: \"LOWLW\",\n",
    "    15: \"CHLFAIL\", 16: \"NAVWARN\", 17: \"ABSAER\", 18: \"SPARE3\", 19: \"TRICHO\",\n",
    "    20: \"SPARE4\", 21: \"MAXAERITER\", 22: \"MODGLINT\", 23: \"CHLWARN\", 24: \"ATMWARN\",\n",
    "    25: \"SPARE5\", 26: \"SEAICE\", 27: \"NAVFAIL\", 28: \"FILTER\", 29: \"SPARE6\",\n",
    "    30: \"SPARE7\", 31: \"HIPOL\"\n",
    "}\n",
    "\n",
    "print(f\"Using BASE_DIR={BASE_DIR}\")\n",
    "print(f\"MAIN_DIR={MAIN_DIR}\")\n",
    "print(f\"SUMMARY_CSV={SUMMARY_CSV}\")\n",
    "print(f\"INSITU_CSV={INSITU_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee8ca128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-23T16:12:25] Found 58 NetCDF files under Y:\\Mingyue\\Lucinda\\l2gen_product\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "  processed 25/58...\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "  processed 50/58...\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "Found variable using alias: 'latitude'\n",
      "Found variable using alias: 'longitude'\n",
      "Found variable using alias: 'chlor_a'\n",
      "Found variable using alias: 'l2_flags'\n",
      "[2025-10-23T16:20:37] Wrote 58 rows to Y:\\Mingyue\\Lucinda\\Lucinda_l2_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: batch extract chlor_a + window stats + UTC acquisition datetime into SUMMARY_CSV ---\n",
    "\n",
    "# ---------------- Utility helpers ----------------\n",
    "\n",
    "def find_variable(data_source, aliases):\n",
    "    \"\"\"Search a dataset or group for the first variable matching any alias.\"\"\"\n",
    "    for alias in aliases:\n",
    "        var = data_source.variables.get(alias)\n",
    "        if var is not None:\n",
    "            print(f\"Found variable using alias: '{alias}'\")\n",
    "            return var\n",
    "    return None\n",
    "\n",
    "def get_var_anywhere(dataset, aliases,\n",
    "                     group_candidates=('navigation_data', 'geophysical_data',\n",
    "                                       'geolocation_data', 'nav', 'navigation', 'geophysical')):\n",
    "    \"\"\"Try to find a variable in common groups first, then at root.\"\"\"\n",
    "    for gname in group_candidates:\n",
    "        grp = dataset.groups.get(gname)\n",
    "        if grp:\n",
    "            v = find_variable(grp, aliases)\n",
    "            if v is not None:\n",
    "                return v\n",
    "    return find_variable(dataset, aliases)\n",
    "\n",
    "def ensure_2d_latlon(lats, lons):\n",
    "    \"\"\"Support both 1D and 2D navigation arrays.\"\"\"\n",
    "    if lats.ndim == 1 and lons.ndim == 1:\n",
    "        return np.meshgrid(lats, lons, indexing='ij')\n",
    "    return lats, lons\n",
    "\n",
    "def window_slice(center_row, center_col, half_size, nrows, ncols):\n",
    "    r0 = max(0, center_row - half_size)\n",
    "    r1 = min(nrows, center_row + half_size + 1)\n",
    "    c0 = max(0, center_col - half_size)\n",
    "    c1 = min(ncols, center_col + half_size + 1)\n",
    "    return r0, r1, c0, c1\n",
    "\n",
    "def window_stats(masked_array, r0, r1, c0, c1):\n",
    "    box = masked_array[r0:r1, c0:c1]\n",
    "    valid_count = int(np.ma.count(box))\n",
    "    total = int((r1 - r0) * (c1 - c0))\n",
    "    if valid_count > 0:\n",
    "        return {\n",
    "            \"mean\": float(np.ma.mean(box)),\n",
    "            \"median\": float(np.ma.median(box)),\n",
    "            \"valid_pixels\": valid_count,\n",
    "            \"total_pixels\": total\n",
    "        }\n",
    "    else:\n",
    "        return {\"mean\": None, \"median\": None, \"valid_pixels\": 0, \"total_pixels\": total}\n",
    "\n",
    "def decode_flags(flag_val):\n",
    "    if flag_val is None or np.ma.is_masked(flag_val):\n",
    "        return []\n",
    "    names = []\n",
    "    ival = int(flag_val)\n",
    "    for bit, name in L2_FLAG_BITS.items():\n",
    "        if (ival >> bit) & 1:\n",
    "            names.append(name)\n",
    "    return names\n",
    "\n",
    "def nearest_unmasked(chl, start_row, start_col, max_radius=50):\n",
    "    \"\"\"Find nearest unmasked, finite chlor_a pixel starting at (row, col).\"\"\"\n",
    "    nrows, ncols = chl.shape\n",
    "    if not np.ma.is_masked(chl[start_row, start_col]) and np.isfinite(chl[start_row, start_col]):\n",
    "        return start_row, start_col, float(chl[start_row, start_col]), 0\n",
    "    for rad in range(1, max_radius + 1):\n",
    "        r0, r1, c0, c1 = window_slice(start_row, start_col, rad, nrows, ncols)\n",
    "        window = chl[r0:r1, c0:c1]\n",
    "        if np.ma.count(window) == 0:\n",
    "            continue\n",
    "        rr, cc = np.mgrid[r0:r1, c0:c1]\n",
    "        dist2 = (rr - start_row) ** 2 + (cc - start_col) ** 2\n",
    "        valid_mask = (~np.ma.getmaskarray(window)) & np.isfinite(window.filled(np.nan))\n",
    "        if not np.any(valid_mask):\n",
    "            continue\n",
    "        dist2_valid = np.where(valid_mask, dist2, np.inf)\n",
    "        flat_idx = np.argmin(dist2_valid)\n",
    "        wr, wc = np.unravel_index(flat_idx, window.shape)\n",
    "        rr_abs, cc_abs = r0 + wr, c0 + wc\n",
    "        val = float(chl[rr_abs, cc_abs])\n",
    "        d = int(np.sqrt(dist2[wr, wc]))\n",
    "        return rr_abs, cc_abs, val, d\n",
    "    return None, None, None, None\n",
    "\n",
    "# ---------------- Rrs helpers ----------------\n",
    "_RRS_RE = re.compile(r'^Rrs_(\\d+)$', flags=re.IGNORECASE)\n",
    "\n",
    "def find_rrs_bands(dataset):\n",
    "    candidates = []\n",
    "    all_containers = [dataset] + [dataset.groups[g] for g in dataset.groups]\n",
    "    for container in all_containers:\n",
    "        for name, var in container.variables.items():\n",
    "            m = _RRS_RE.match(name)\n",
    "            if m:\n",
    "                wl = int(m.group(1))\n",
    "                candidates.append((wl, var))\n",
    "    candidates.sort(key=lambda x: x[0])\n",
    "    return candidates\n",
    "\n",
    "def read_rrs_at(rrs_vars, row, col):\n",
    "    spectrum = {}\n",
    "    for wl, var in rrs_vars:\n",
    "        val = var[row, col]\n",
    "        spectrum[wl] = None if np.ma.is_masked(val) or not np.isfinite(val) else float(val)\n",
    "    return spectrum\n",
    "\n",
    "# ---------------- Datetime extraction helpers ----------------\n",
    "_FILENAME_DT_RE = re.compile(r'(\\d{8}T\\d{6})')\n",
    "\n",
    "def _parse_iso_like(s: str) -> Optional[datetime]:\n",
    "    if not s:\n",
    "        return None\n",
    "    s = s.strip()\n",
    "    if s.endswith(\"Z\"):\n",
    "        s = s[:-1] + \"+00:00\"\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(s)\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        return dt.astimezone(timezone.utc)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _extract_datetime_from_attrs(ds) -> Optional[datetime]:\n",
    "    attr_candidates = [\n",
    "        \"time_coverage_start\", \"time_coverage_center\",\n",
    "        \"start_time\", \"StartTime\", \"start_datetime\",\n",
    "        \"start_date\", \"product_start_time\", \"isotime\", \"isodate\"\n",
    "    ]\n",
    "    for k in attr_candidates:\n",
    "        if k in ds.__dict__:\n",
    "            dt = _parse_iso_like(str(getattr(ds, k)))\n",
    "            if dt:\n",
    "                return dt\n",
    "    # DOY-based triplet: start_year + start_day (+ start_msec or start_sec)\n",
    "    if all(hasattr(ds, x) for x in (\"start_year\", \"start_day\")):\n",
    "        try:\n",
    "            year = int(getattr(ds, \"start_year\"))\n",
    "            doy  = int(getattr(ds, \"start_day\"))\n",
    "            base = datetime(year, 1, 1, tzinfo=timezone.utc) + timedelta(days=doy - 1)\n",
    "            if hasattr(ds, \"start_msec\"):\n",
    "                msec = float(getattr(ds, \"start_msec\"))\n",
    "                base = base + timedelta(milliseconds=msec)\n",
    "            elif hasattr(ds, \"start_sec\"):\n",
    "                sec = float(getattr(ds, \"start_sec\"))\n",
    "                base = base + timedelta(seconds=sec)\n",
    "            return base\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def _extract_datetime_from_timevar(ds) -> Optional[datetime]:\n",
    "    if \"time\" not in ds.variables:\n",
    "        return None\n",
    "    tv = ds.variables[\"time\"]\n",
    "    try:\n",
    "        t0 = tv[()] if tv.shape == () else tv[0]\n",
    "        units = getattr(tv, \"units\", None)\n",
    "        calendar = getattr(tv, \"calendar\", \"standard\")\n",
    "        if units:\n",
    "            dt = num2date(t0, units=units, calendar=calendar)\n",
    "            if isinstance(dt, np.ndarray):\n",
    "                dt = dt.item()\n",
    "            if getattr(dt, \"tzinfo\", None) is None:\n",
    "                dt = dt.replace(tzinfo=timezone.utc)\n",
    "            return dt.astimezone(timezone.utc)\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def _extract_datetime_from_filename(nc_path: Path) -> Optional[datetime]:\n",
    "    m = _FILENAME_DT_RE.search(nc_path.name)\n",
    "    if not m:\n",
    "        return None\n",
    "    token = m.group(1)  # YYYYMMDDTHHMMSS\n",
    "    try:\n",
    "        return datetime.strptime(token, \"%Y%m%dT%H%M%S\").replace(tzinfo=timezone.utc)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_acq_datetime_utc(nc_path: Path) -> Optional[datetime]:\n",
    "    \"\"\"Best-effort extraction of acquisition datetime (UTC).\"\"\"\n",
    "    try:\n",
    "        with netcdf.Dataset(str(nc_path), \"r\") as ds:\n",
    "            dt = _extract_datetime_from_attrs(ds)\n",
    "            if dt:\n",
    "                return dt\n",
    "            dt = _extract_datetime_from_timevar(ds)\n",
    "            if dt:\n",
    "                return dt\n",
    "    except Exception:\n",
    "        pass\n",
    "    return _extract_datetime_from_filename(nc_path)\n",
    "\n",
    "# ---------------- Row flattener & neighborhood flags ----------------\n",
    "def _collect_neighborhood_flags(flags_var, row, col, compute_windows, nrows, ncols):\n",
    "    neighborhood_flags = {}\n",
    "    if flags_var is None:\n",
    "        return {f\"{w}x{w}\": [] for w in compute_windows}\n",
    "    flags = flags_var[:]\n",
    "    for w in compute_windows:\n",
    "        half = (w - 1) // 2\n",
    "        r0, r1, c0, c1 = window_slice(row, col, half, nrows, ncols)\n",
    "        if flags.ndim == 2:\n",
    "            fwin = flags[r0:r1, c0:c1].astype(int)\n",
    "            names = []\n",
    "            for fv in np.unique(fwin):\n",
    "                names.extend(decode_flags(fv))\n",
    "            seen = set()\n",
    "            uniq = [x for x in names if not (x in seen or seen.add(x))]\n",
    "            neighborhood_flags[f\"{w}x{w}\"] = uniq\n",
    "        else:\n",
    "            neighborhood_flags[f\"{w}x{w}\"] = []\n",
    "    return neighborhood_flags\n",
    "\n",
    "def _safe_join_flags(names):\n",
    "    if not names:\n",
    "        return \"\"\n",
    "    seen = set()\n",
    "    uniq = [x for x in names if not (x in seen or seen.add(x))]\n",
    "    return \"|\".join(uniq)\n",
    "\n",
    "def _row_from_result(nc_path: Path, result: dict, acq_dt: Optional[datetime]) -> dict:\n",
    "    win_keys = [\"3x3\", \"5x5\"]\n",
    "    if acq_dt:\n",
    "        acq_iso  = acq_dt.isoformat()\n",
    "        acq_date = acq_dt.date().isoformat()\n",
    "        acq_time = acq_dt.time().isoformat(timespec=\"seconds\")\n",
    "    else:\n",
    "        acq_iso = acq_date = acq_time = \"\"\n",
    "\n",
    "    row = {\n",
    "        \"file\": str(nc_path),\n",
    "        \"filename\": nc_path.name,\n",
    "        \"acq_datetime_utc\": acq_iso,\n",
    "        \"acq_date_utc\": acq_date,\n",
    "        \"acq_time_utc\": acq_time,\n",
    "        \"nearest_row\": result[\"nearest_pixel_index\"][\"row\"],\n",
    "        \"nearest_col\": result[\"nearest_pixel_index\"][\"col\"],\n",
    "        \"nearest_lat\": result[\"nearest_pixel_geo\"][\"lat\"],\n",
    "        \"nearest_lon\": result[\"nearest_pixel_geo\"][\"lon\"],\n",
    "        \"center_chlor_a\": result[\"center_value\"],\n",
    "        \"center_flags\": _safe_join_flags(result.get(\"center_flags\", [])),\n",
    "    }\n",
    "    for wk in win_keys:\n",
    "        ws = result[\"window_stats\"].get(wk, {})\n",
    "        row[f\"{wk}_mean\"]   = ws.get(\"mean\")\n",
    "        row[f\"{wk}_median\"] = ws.get(\"median\")\n",
    "        row[f\"{wk}_valid\"]  = ws.get(\"valid_pixels\")\n",
    "        row[f\"{wk}_total\"]  = ws.get(\"total_pixels\")\n",
    "        nfl = result.get(\"neighborhood_flags\", {}).get(wk, [])\n",
    "        row[f\"{wk}_flags\"] = _safe_join_flags(nfl)\n",
    "\n",
    "    nv = result.get(\"nearest_valid_if_center_masked\")\n",
    "    if nv is not None:\n",
    "        row[\"nearest_valid_value\"]             = nv.get(\"value\")\n",
    "        row[\"nearest_valid_row\"]               = nv.get(\"row\")\n",
    "        row[\"nearest_valid_col\"]               = nv.get(\"col\")\n",
    "        row[\"nearest_valid_distance_pixels\"]   = nv.get(\"distance_pixels\")\n",
    "        row[\"nearest_valid_flags\"]             = _safe_join_flags(nv.get(\"flags\", []))\n",
    "    else:\n",
    "        row[\"nearest_valid_value\"]           = None\n",
    "        row[\"nearest_valid_row\"]             = None\n",
    "        row[\"nearest_valid_col\"]             = None\n",
    "        row[\"nearest_valid_distance_pixels\"] = None\n",
    "        row[\"nearest_valid_flags\"]           = \"\"\n",
    "    return row\n",
    "\n",
    "# ---------------- Main extractor ----------------\n",
    "def get_chlor_a_at_location(\n",
    "    l2_file_path: str,\n",
    "    target_lat: float,\n",
    "    target_lon: float,\n",
    "    compute_windows=(3, 5),\n",
    "    return_nearest_if_center_masked: bool = True\n",
    "):\n",
    "    if not os.path.exists(l2_file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {l2_file_path}\")\n",
    "\n",
    "    with netcdf.Dataset(l2_file_path, 'r') as ds:\n",
    "        lats_var  = get_var_anywhere(ds, LAT_ALIASES)\n",
    "        lons_var  = get_var_anywhere(ds, LON_ALIASES)\n",
    "        chl_var   = get_var_anywhere(ds, CHLOR_A_ALIASES)\n",
    "        flags_var = get_var_anywhere(ds, FLAGS_ALIASES)\n",
    "\n",
    "        if lats_var is None or lons_var is None:\n",
    "            raise KeyError(f\"Could not find latitude/longitude using {LAT_ALIASES} / {LON_ALIASES}\")\n",
    "        if chl_var is None:\n",
    "            raise KeyError(f\"Could not find chlorophyll-a using {CHLOR_A_ALIASES}\")\n",
    "\n",
    "        lats = lats_var[:]\n",
    "        lons = lons_var[:]\n",
    "        chl  = chl_var[:]\n",
    "\n",
    "        lats, lons = ensure_2d_latlon(lats, lons)\n",
    "        nrows, ncols = lats.shape\n",
    "\n",
    "        lon_min, lon_max = np.nanmin(lons), np.nanmax(lons)\n",
    "        t_lon = target_lon\n",
    "        if (lon_max - lon_min) > 300:  # handle dateline\n",
    "            candidates = [t_lon, t_lon - 360, t_lon + 360]\n",
    "            diffs = [min(abs(c - lon_min), abs(c - lon_max)) for c in candidates]\n",
    "            t_lon = candidates[int(np.argmin(diffs))]\n",
    "\n",
    "        dist_sq = (lats - target_lat) ** 2 + (lons - t_lon) ** 2\n",
    "        min_idx = np.argmin(dist_sq)\n",
    "        row, col = np.unravel_index(min_idx, lats.shape)\n",
    "\n",
    "        chl = np.ma.array(chl, copy=False)\n",
    "        center_val = chl[row, col]\n",
    "        center_val_out = None if np.ma.is_masked(center_val) or not np.isfinite(center_val) else float(center_val)\n",
    "\n",
    "        center_flags = None\n",
    "        if flags_var is not None:\n",
    "            flags = flags_var[:]\n",
    "            if flags.ndim == 2 and 0 <= row < flags.shape[0] and 0 <= col < flags.shape[1]:\n",
    "                center_flags = int(flags[row, col])\n",
    "            elif flags.ndim == 1:\n",
    "                center_flags = int(min_idx)\n",
    "        decoded_center_flags = decode_flags(center_flags) if center_flags is not None else []\n",
    "\n",
    "        stats_by_window = {}\n",
    "        for w in compute_windows:\n",
    "            half = (w - 1) // 2\n",
    "            r0, r1, c0, c1 = window_slice(row, col, half, nrows, ncols)\n",
    "            stats_by_window[f\"{w}x{w}\"] = window_stats(chl, r0, r1, c0, c1)\n",
    "\n",
    "        nearest_info = None\n",
    "        nr = cc = nval = nd = None\n",
    "        if center_val_out is None and return_nearest_if_center_masked:\n",
    "            nr, cc, nval, nd = nearest_unmasked(chl, row, col, max_radius=50)\n",
    "            if nr is not None:\n",
    "                nearest_flags = None\n",
    "                if flags_var is not None:\n",
    "                    flags = flags_var[:]\n",
    "                    if flags.ndim == 2:\n",
    "                        nearest_flags = int(flags[nr, cc])\n",
    "                    elif flags.ndim == 1:\n",
    "                        nearest_flags = int(nr * ncols + cc)\n",
    "                nearest_info = {\n",
    "                    \"row\": int(nr),\n",
    "                    \"col\": int(cc),\n",
    "                    \"value\": float(nval),\n",
    "                    \"distance_pixels\": int(nd),\n",
    "                    \"flags\": decode_flags(nearest_flags) if nearest_flags is not None else []\n",
    "                }\n",
    "\n",
    "        rrs_vars = find_rrs_bands(ds)\n",
    "        rrs_center  = read_rrs_at(rrs_vars, row, col) if rrs_vars else {}\n",
    "        rrs_nearest = None\n",
    "        if nr is not None and cc is not None and rrs_vars:\n",
    "            rrs_nearest = read_rrs_at(rrs_vars, nr, cc)\n",
    "\n",
    "        return {\n",
    "            \"target_input\": {\"lat\": float(target_lat), \"lon\": float(target_lon)},\n",
    "            \"nearest_pixel_index\": {\"row\": int(row), \"col\": int(col)},\n",
    "            \"nearest_pixel_geo\": {\"lat\": float(lats[row, col]), \"lon\": float(lons[row, col])},\n",
    "            \"center_value\": center_val_out,\n",
    "            \"window_stats\": stats_by_window,\n",
    "            \"center_flags\": decoded_center_flags,\n",
    "            \"neighborhood_flags\": _collect_neighborhood_flags(flags_var, row, col, compute_windows, nrows, ncols),\n",
    "            \"nearest_valid_if_center_masked\": nearest_info,\n",
    "            \"rrs_center\": rrs_center,\n",
    "            \"rrs_nearest_valid\": rrs_nearest\n",
    "        }\n",
    "\n",
    "def process_folder(\n",
    "    main_dir: Path,\n",
    "    target_lat: float,\n",
    "    target_lon: float,\n",
    "    out_csv: Path,\n",
    "    pattern: str = \"*.nc\",\n",
    "    compute_windows=(3, 5),\n",
    "    return_nearest_if_center_masked=True,\n",
    "    recursive=True\n",
    "):\n",
    "    main_dir = Path(main_dir)\n",
    "    assert main_dir.exists(), f\"Main dir not found: {main_dir}\"\n",
    "\n",
    "    paths = sorted(main_dir.rglob(pattern) if recursive else main_dir.glob(pattern))\n",
    "    rows, errors = [], []\n",
    "\n",
    "    print(f\"[{datetime.now().isoformat(timespec='seconds')}] Found {len(paths)} NetCDF files under {main_dir}\")\n",
    "    for i, p in enumerate(paths, 1):\n",
    "        try:\n",
    "            acq_dt = extract_acq_datetime_utc(p)\n",
    "            result = get_chlor_a_at_location(\n",
    "                str(p),\n",
    "                target_lat,\n",
    "                target_lon,\n",
    "                compute_windows=compute_windows,\n",
    "                return_nearest_if_center_masked=return_nearest_if_center_masked\n",
    "            )\n",
    "            row = _row_from_result(p, result, acq_dt)\n",
    "            rows.append(row)\n",
    "            if i % 25 == 0:\n",
    "                print(f\"  processed {i}/{len(paths)}...\")\n",
    "        except Exception as e:\n",
    "            errors.append((str(p), repr(e)))\n",
    "            print(f\"  ERROR on {p.name}: {e!r}\")\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty:\n",
    "        df.sort_values(by=[\"acq_datetime_utc\", \"filename\"], inplace=True, ignore_index=True)\n",
    "\n",
    "    out_csv = Path(out_csv)\n",
    "    out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"[{datetime.now().isoformat(timespec='seconds')}] Wrote {len(df)} rows to {out_csv}\")\n",
    "\n",
    "    if errors:\n",
    "        print(\"\\nFiles with errors:\")\n",
    "        for f, msg in errors[:20]:\n",
    "            print(f\"  {f} -> {msg}\")\n",
    "        if len(errors) > 20:\n",
    "            print(f\"  ... and {len(errors)-20} more\")\n",
    "\n",
    "    return df, errors\n",
    "\n",
    "# --------- Run the batch extraction ---------\n",
    "df, errs = process_folder(\n",
    "    main_dir=MAIN_DIR,\n",
    "    target_lat=TARGET_LAT,\n",
    "    target_lon=TARGET_LON,\n",
    "    out_csv=SUMMARY_CSV,\n",
    "    pattern=NC_GLOB_PATTERN,\n",
    "    compute_windows=(3, 5),\n",
    "    return_nearest_if_center_masked=True,\n",
    "    recursive=RECURSIVE_SEARCH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3c8abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Cell 3: robust in-situ matchup (±HOURS_TOL hours) and write back to SUMMARY_CSV ---\n",
    "\n",
    "# # Load satellite summary\n",
    "# df_sat = pd.read_csv(SUMMARY_CSV)\n",
    "# df_sat.columns = [c.strip() for c in df_sat.columns]\n",
    "\n",
    "# # Parse acquisition datetime + fallback from separate date/time cols\n",
    "# df_sat['acq_datetime_utc'] = pd.to_datetime(df_sat.get('acq_datetime_utc'), utc=True, errors='coerce')\n",
    "\n",
    "# if 'acq_date_utc' in df_sat.columns and 'acq_time_utc' in df_sat.columns:\n",
    "#     missing_mask = df_sat['acq_datetime_utc'].isna()\n",
    "#     if missing_mask.any():\n",
    "#         dt_fallback = pd.to_datetime(\n",
    "#             df_sat.loc[missing_mask, 'acq_date_utc'].astype(str).str.strip() + \" \" +\n",
    "#             df_sat.loc[missing_mask, 'acq_time_utc'].astype(str).str.strip(),\n",
    "#             utc=True, errors='coerce'\n",
    "#         )\n",
    "#         df_sat.loc[missing_mask, 'acq_datetime_utc'] = dt_fallback\n",
    "\n",
    "# # Load in-situ\n",
    "# df_ins = pd.read_csv(INSITU_CSV)\n",
    "# df_ins.columns = [c.strip() for c in df_ins.columns]\n",
    "\n",
    "# # Detect column names\n",
    "# date_col = next((c for c in df_ins.columns if c.lower() == 'date'), None)\n",
    "# time_col = next((c for c in df_ins.columns if c.lower() == 'time'), None)\n",
    "# chl_col  = next((c for c in df_ins.columns if c.lower() in {'chlorophyll-a','chlorophyll_a','chl_a','chl'}), None)\n",
    "\n",
    "# if date_col is None or time_col is None or chl_col is None:\n",
    "#     raise RuntimeError(\n",
    "#         f\"Could not find expected columns in {INSITU_CSV}. \"\n",
    "#         f\"Have={list(df_ins.columns)}; need Date, Time, and Chlorophyll-a.\"\n",
    "#     )\n",
    "\n",
    "# # Build UTC timestamps (adjust here if in-situ is in local time)\n",
    "# df_ins['insitu_datetime_utc'] = pd.to_datetime(\n",
    "#     df_ins[date_col].astype(str).str.strip() + \" \" + df_ins[time_col].astype(str).str.strip(),\n",
    "#     utc=True, errors='coerce'\n",
    "# )\n",
    "# df_ins['in-situ chl-a'] = pd.to_numeric(df_ins[chl_col], errors='coerce')\n",
    "# df_ins = df_ins.dropna(subset=['insitu_datetime_utc']).copy()\n",
    "\n",
    "# # Split satellite rows by timestamp availability\n",
    "# sat_valid   = df_sat.dropna(subset=['acq_datetime_utc']).copy()\n",
    "# sat_missing = df_sat[df_sat['acq_datetime_utc'].isna()].copy()\n",
    "\n",
    "# # Sort (required for merge_asof)\n",
    "# sat_valid  = sat_valid.sort_values('acq_datetime_utc').reset_index(drop=True)\n",
    "# ins_sorted = df_ins.sort_values('insitu_datetime_utc').reset_index(drop=True)\n",
    "\n",
    "# # Nearest-time join\n",
    "# if not sat_valid.empty and not ins_sorted.empty:\n",
    "#     merged = pd.merge_asof(\n",
    "#         sat_valid,\n",
    "#         ins_sorted[['insitu_datetime_utc', 'in-situ chl-a']].rename(columns={'insitu_datetime_utc':'match_time'}),\n",
    "#         left_on='acq_datetime_utc',\n",
    "#         right_on='match_time',\n",
    "#         direction='nearest',\n",
    "#         tolerance=pd.Timedelta(hours=HOURS_TOL)\n",
    "#     ).drop(columns=['match_time'])\n",
    "# else:\n",
    "#     merged = sat_valid.copy()\n",
    "#     merged['in-situ chl-a'] = np.nan\n",
    "\n",
    "# # Unmatchable rows keep NaN\n",
    "# if not sat_missing.empty:\n",
    "#     sat_missing = sat_missing.copy()\n",
    "#     sat_missing['in-situ chl-a'] = np.nan\n",
    "\n",
    "# # Stitch + sort\n",
    "# out = pd.concat([merged, sat_missing], ignore_index=True)\n",
    "# sort_keys = [k for k in ['acq_datetime_utc','filename'] if k in out.columns]\n",
    "# if sort_keys:\n",
    "#     out = out.sort_values(sort_keys).reset_index(drop=True)\n",
    "\n",
    "# # Save back to SUMMARY_CSV\n",
    "# out.to_csv(SUMMARY_CSV, index=False)\n",
    "\n",
    "# # Report\n",
    "# n_total     = len(out)\n",
    "# n_missing_t = sat_missing.shape[0]\n",
    "# n_matched   = out['in-situ chl-a'].notna().sum()\n",
    "# print(f\"Matched in-situ chl-a within ±{HOURS_TOL} h for {n_matched}/{n_total} satellite rows.\")\n",
    "# print(f\"Rows lacking satellite timestamps (could not be matched): {n_missing_t}\")\n",
    "# print(f\"Updated file: {SUMMARY_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8a57b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched AERONET-OC within ±3 h for 42/58 satellite rows.\n",
      "Satellite rows with missing timestamps (unmatchable): 0\n",
      "New file written: Y:\\Mingyue\\Lucinda\\Lucinda_l2_summary_2.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -- coding: utf-8 --\n",
    "\n",
    "\n",
    "# OUTPUT (will NOT overwrite inputs)\n",
    "\n",
    "OUT_MATCHED_CSV = BASE_DIR / f\"{SITE}_l2_summary_2.csv\"\n",
    "\n",
    "# Optional AERONET Rrs columns to carry through if present\n",
    "AOC_RRS_COLS = [\n",
    "    \"aoc_rrs412\",\"aoc_rrs443\",\"aoc_rrs490\",\"aoc_rrs510\",\n",
    "    \"aoc_rrs560\",\"aoc_rrs665\",\"aoc_rrs681\"\n",
    "]\n",
    "# ------------------------------------------------\n",
    "\n",
    "def parse_satellite_times(df_sat: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_sat = df_sat.copy()\n",
    "    df_sat.columns = [c.strip() for c in df_sat.columns]\n",
    "\n",
    "    # Preferred combined datetime\n",
    "    if \"acq_datetime_utc\" in df_sat.columns:\n",
    "        df_sat[\"acq_datetime_utc\"] = pd.to_datetime(\n",
    "            df_sat[\"acq_datetime_utc\"], utc=True, errors=\"coerce\"\n",
    "        )\n",
    "    else:\n",
    "        df_sat[\"acq_datetime_utc\"] = pd.NaT\n",
    "\n",
    "    # Fallback from separate date/time if needed\n",
    "    if \"acq_date_utc\" in df_sat.columns and \"acq_time_utc\" in df_sat.columns:\n",
    "        miss = df_sat[\"acq_datetime_utc\"].isna()\n",
    "        if miss.any():\n",
    "            dt = pd.to_datetime(\n",
    "                df_sat.loc[miss, \"acq_date_utc\"].astype(str).str.strip() + \" \" +\n",
    "                df_sat.loc[miss, \"acq_time_utc\"].astype(str).str.strip(),\n",
    "                utc=True, errors=\"coerce\"\n",
    "            )\n",
    "            df_sat.loc[miss, \"acq_datetime_utc\"] = dt\n",
    "\n",
    "    return df_sat\n",
    "\n",
    "def load_insitu_aoc(df_ins: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_ins = df_ins.copy()\n",
    "    df_ins.columns = [c.strip() for c in df_ins.columns]\n",
    "\n",
    "    # Required AERONET-OC columns\n",
    "    required = [\"aoc_datetime\", \"aoc_chl_a\"]\n",
    "    for col in required:\n",
    "        if col not in df_ins.columns:\n",
    "            raise RuntimeError(f\"In-situ file missing required column '{col}'\")\n",
    "\n",
    "    df_ins[\"aoc_datetime\"] = pd.to_datetime(df_ins[\"aoc_datetime\"], utc=True, errors=\"coerce\")\n",
    "    df_ins[\"aoc_chl_a\"]    = pd.to_numeric(df_ins[\"aoc_chl_a\"],    errors=\"coerce\")\n",
    "    df_ins = df_ins.dropna(subset=[\"aoc_datetime\"]).reset_index(drop=True)\n",
    "\n",
    "    keep_cols = [\"aoc_site\",\"aoc_latitude\",\"aoc_longitude\",\"aoc_datetime\",\"aoc_chl_a\"]\n",
    "    keep_cols += [c for c in AOC_RRS_COLS if c in df_ins.columns]\n",
    "    df_ins = df_ins[[c for c in keep_cols if c in df_ins.columns]].copy()\n",
    "    return df_ins\n",
    "\n",
    "def main():\n",
    "    if not SUMMARY_CSV.exists():\n",
    "        raise FileNotFoundError(f\"Satellite summary not found: {SUMMARY_CSV}\")\n",
    "    if not INSITU_CSV.exists():\n",
    "        raise FileNotFoundError(f\"In-situ CSV not found: {INSITU_CSV}\")\n",
    "\n",
    "    df_sat_raw = pd.read_csv(SUMMARY_CSV)\n",
    "    df_ins_raw = pd.read_csv(INSITU_CSV)\n",
    "\n",
    "    # Prepare tables\n",
    "    df_sat = parse_satellite_times(df_sat_raw)\n",
    "    df_ins = load_insitu_aoc(df_ins_raw)\n",
    "\n",
    "    # Split satellite rows by timestamp availability\n",
    "    sat_valid   = df_sat.dropna(subset=[\"acq_datetime_utc\"]).copy()\n",
    "    sat_missing = df_sat[df_sat[\"acq_datetime_utc\"].isna()].copy()\n",
    "\n",
    "    # Sort for merge_asof\n",
    "    sat_valid  = sat_valid.sort_values(\"acq_datetime_utc\").reset_index(drop=True)\n",
    "    ins_sorted = df_ins.sort_values(\"aoc_datetime\").reset_index(drop=True)\n",
    "\n",
    "    # Nearest-time join within ± HOURS_TOL\n",
    "    if not sat_valid.empty and not ins_sorted.empty:\n",
    "        right_cols = [\"aoc_datetime\",\"aoc_chl_a\"] + [c for c in AOC_RRS_COLS if c in ins_sorted.columns]\n",
    "        right = ins_sorted[right_cols].rename(columns={\"aoc_datetime\":\"match_time\"})\n",
    "\n",
    "        merged = pd.merge_asof(\n",
    "            sat_valid,\n",
    "            right,\n",
    "            left_on=\"acq_datetime_utc\",\n",
    "            right_on=\"match_time\",\n",
    "            direction=\"nearest\",\n",
    "            tolerance=pd.Timedelta(hours=HOURS_TOL)\n",
    "        )\n",
    "\n",
    "        merged[\"match_dt_hours\"] = (\n",
    "            (merged[\"match_time\"] - merged[\"acq_datetime_utc\"]).dt.total_seconds() / 3600.0\n",
    "        )\n",
    "\n",
    "        merged = merged.rename(columns={\n",
    "            \"match_time\": \"matched_aoc_datetime\",\n",
    "            \"aoc_chl_a\":  \"matched_aoc_chl_a\"\n",
    "        })\n",
    "\n",
    "        # Prefix any carried Rrs columns\n",
    "        for c in AOC_RRS_COLS:\n",
    "            if c in merged.columns:\n",
    "                merged = merged.rename(columns={c: f\"matched_{c}\"})\n",
    "    else:\n",
    "        merged = sat_valid.copy()\n",
    "        merged[\"matched_aoc_datetime\"] = pd.NaT\n",
    "        merged[\"matched_aoc_chl_a\"]    = np.nan\n",
    "        merged[\"match_dt_hours\"]       = np.nan\n",
    "\n",
    "    # Rows without satellite timestamps remain unmatched\n",
    "    if not sat_missing.empty:\n",
    "        sat_missing = sat_missing.copy()\n",
    "        sat_missing[\"matched_aoc_datetime\"] = pd.NaT\n",
    "        sat_missing[\"matched_aoc_chl_a\"]    = np.nan\n",
    "        sat_missing[\"match_dt_hours\"]       = np.nan\n",
    "\n",
    "    # Stitch & sort\n",
    "    out = pd.concat([merged, sat_missing], ignore_index=True)\n",
    "    sort_keys = [k for k in [\"acq_datetime_utc\",\"filename\",\"file\"] if k in out.columns]\n",
    "    if sort_keys:\n",
    "        out = out.sort_values(sort_keys).reset_index(drop=True)\n",
    "\n",
    "    # Write new file\n",
    "    OUT_MATCHED_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out.to_csv(OUT_MATCHED_CSV, index=False)\n",
    "\n",
    "    # Report\n",
    "    n_total   = len(out)\n",
    "    n_matched = out[\"matched_aoc_chl_a\"].notna().sum()\n",
    "    n_no_time = sat_missing.shape[0]\n",
    "    print(f\"Matched AERONET-OC within ±{HOURS_TOL} h for {n_matched}/{n_total} satellite rows.\")\n",
    "    print(f\"Satellite rows with missing timestamps (unmatchable): {n_no_time}\")\n",
    "    print(f\"New file written: {OUT_MATCHED_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seadas (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
